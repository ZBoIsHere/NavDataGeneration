## PYTHON 
## ******************************************************************** ##
## author: CTO_TI_FBSYJG
## create time: 2025/09/23 12:03:01 GMT+08:00
## ******************************************************************** ##
import sys
import os
import uuid
import argparse
import json
import gzip
import logging

logger = logging.getLogger(__name__)

logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s')

#os.chdir('/mnt/sfs-turbo-workflow/data-platform/')

# UUID ç”Ÿæˆé€»è¾‘
def generate_task_uuid(scene_type, scene_name, split, start_position, start_rotation, object_category):
    namespace = uuid.NAMESPACE_DNS
    key = f"{scene_type}:{scene_name}:{split}:{start_position}:{start_rotation}:{object_category}"
    return str(uuid.uuid5(namespace, key))

# UUID ç”Ÿæˆé€»è¾‘
def generate_traj_uuid(scene_type, scene_name, split, start_position, start_rotation, object_category, gen_traj_method, experiment_name):
    namespace = uuid.NAMESPACE_DNS
    key = f"{scene_type}:{scene_name}:{split}:{start_position}:{start_rotation}:{object_category}:{gen_traj_method}:{experiment_name}"
    return str(uuid.uuid5(namespace, key))

# ä¸»ç¨‹åºé€»è¾‘
def main(args):
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_path, exist_ok=True)
    filter_ids = args.filter_ids.split(",")
    logger.info(f"ğŸ” ä¸€å…±è¿‡æ»¤çš„uuidæ•°é‡ä¸º: {len(filter_ids)}")

    for file_name in os.listdir(args.input_path):
        if file_name.startswith(args.scene_name) and file_name.endswith(".json.gz"):
            input_file_path = os.path.join(args.input_path, file_name)
            output_file_path = os.path.join(args.output_path, file_name)
            # å¦‚æœè¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ä¸”ä¸è¦†ç›–ï¼Œåˆ™è·³è¿‡
            if os.path.exists(output_file_path) and args.overwrite.lower() != "true":
                logger.info(f"âš ï¸  è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ä¸”æœªè®¾ç½®è¦†ç›–ï¼Œè·³è¿‡æ–‡ä»¶: {output_file_path}")
                continue
            logger.info(f"ğŸ“‚ å¤„ç†æ–‡ä»¶: {input_file_path}")
            with gzip.open(input_file_path, 'rt', encoding='utf-8') as f:
                data = json.load(f)
                filtered_episodes = []
                for episode in data["episodes"]:
                    new_uuid = None
                    if args.dataset_type == "traj_datasets":
                        new_uuid = generate_traj_uuid(
                            args.scene_type,
                            args.scene_name,
                            args.split,
                            episode["start_position"],
                            episode["start_rotation"],
                            episode["object_category"],
                            args.gen_traj_method,
                            args.experiment_name
                        )
                    elif args.dataset_type == "task_datasets":
                        new_uuid = generate_task_uuid(
                            args.scene_type,
                            args.scene_name,
                            args.split,
                            episode["start_position"],
                            episode["start_rotation"],
                            episode["object_category"]
                        )
                    if new_uuid in filter_ids:
                        logger.info(f"âœ… ä¿ç•™ episode_id: {episode['episode_id']}, uuid: {new_uuid}")
                        filtered_episodes.append(episode)
                    else:
                        print(f"âŒ åˆ é™¤ episode_id: {episode['episode_id']}, uuid: {new_uuid}")
                data["episodes"] = filtered_episodes
                with gzip.open(output_file_path, "wt", encoding="utf-8") as out_f:
                    json.dump(data, out_f)

                json_output_file_path = output_file_path.replace(".json.gz", ".json")
                with open(json_output_file_path, "w", encoding="utf-8") as json_out_f:
                    json.dump(data, json_out_f, indent=4)
            logger.info(f"âœ… æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_file_path}")

def precheck_args(args):
    invalid_args = []
    if args.dataset_type not in ["task_datasets", "traj_datasets"]:
        logger.warning(f"âŒ å‚æ•° dataset_type {args.dataset_type} å¿…é¡»æ˜¯ 'task_datasets' æˆ– 'traj_datasets'")
        invalid_args.append("dataset_type")
    if not os.path.isdir(args.input_path):
        logger.warning(f"âŒ å‚æ•° input_path {args.input_path} æŒ‡å®šçš„è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•: {args.input_path}")
        invalid_args.append("input_path")

    from pathlib import Path

    # å¦‚æœdataset_typeæ˜¯task_datasets, input_pathé‡Œé¢å¿…é¡»åŒ…å«task_datasetsæˆ–traj_datasets
    if args.dataset_type == "task_datasets":
        if not any(part in ["task_datasets", "traj_datasets"] for part in Path(args.input_path).parts):
            logger.warning(f"âŒ å‚æ•° input_path {args.input_path} è·¯å¾„ä¸­å¿…é¡»åŒ…å« 'task_datasets' æˆ– 'traj_datasets'")
            invalid_args.append("input_path")
    # å¦‚æœdataset_typeæ˜¯traj_datasets, input_pathé‡Œé¢å¿…é¡»åŒ…å«traj
    elif args.dataset_type == "traj_datasets":
        # æ£€æŸ¥ input_path, è·¯å¾„é‡Œå¿…é¡»åŒ…å«trajçš„å­å­—ç¬¦ä¸²
        if not any("traj" in part for part in Path(args.input_path).parts):
            logger.warning(f"âŒ å‚æ•° input_path {args.input_path} è·¯å¾„ä¸­å¿…é¡»åŒ…å« 'traj' å­å­—ç¬¦ä¸²")
            invalid_args.append("input_path")

    # ä¿®æ”¹æˆæ£€æŸ¥args.input_pathçš„å­ç›®å½•çš„å­å­—ç¬¦ä¸²åŒ…å«args.scene_type 
    is_scene_type_valid = False
    for part in Path(args.input_path).parts:
        if args.scene_type in part:
            is_scene_type_valid = True
            break
    
    if not is_scene_type_valid:
        logger.warning(f"âŒ å‚æ•° input_path {args.input_path} è·¯å¾„ä¸­å¿…é¡»åŒ…å« scene_type çš„å­å­—ç¬¦ä¸²: {args.scene_type}")
        invalid_args.append("input_path")


    # æ£€æŸ¥ input_path, è·¯å¾„é‡Œå¿…é¡»åŒ…å«args.splitçš„å­ç›®å½•   
    if args.split not in Path(args.input_path).parts:
        logger.warning(f"âŒ å‚æ•° input_path {args.input_path} è·¯å¾„ä¸­å¿…é¡»åŒ…å« split çš„å­ç›®å½•: {args.split}")
        invalid_args.append("input_path")

    # æ£€æŸ¥ gen_traj_method, ä»…å½“ dataset_type æ˜¯ traj_datasets æ—¶æ£€æŸ¥
    # input_path é‡Œå¿…é¡»åŒ…å« gen_traj_method çš„å­å­—ç¬¦ä¸²
    if args.dataset_type == "traj_datasets":
        if args.gen_traj_method not in args.input_path:
            logger.warning(f"âŒ å‚æ•° input_path {args.input_path} è·¯å¾„ä¸­å¿…é¡»åŒ…å« gen_traj_method: {args.gen_traj_method}")
            invalid_args.append("input_path")
        # å¦‚æœinput_pathé‡Œé¢åŒ…å«traj_datasetsï¼Œé‚£ä¹ˆexperiment_nameå¿…é¡»ä¸º"null"
        if "traj_datasets" in Path(args.input_path).parts and args.experiment_name != "null":
            logger.warning(f"âŒ å‚æ•° experiment_name å¿…é¡»ä¸ºnullï¼Œå› ä¸º input_path {args.input_path} åŒ…å« 'traj_datasets'")
            invalid_args.append("experiment_name")
        # å¦‚æœinput_pathé‡Œé¢ä¸åŒ…å«traj_datasetsï¼Œé‚£ä¹ˆexperiment_nameä¸èƒ½ä¸º"null"
        if "traj_datasets" not in Path(args.input_path).parts and args.experiment_name == "null":
            logger.warning(f"âŒ å‚æ•° experiment_name ä¸èƒ½ä¸ºnullï¼Œå› ä¸º input_path {args.input_path} ä¸åŒ…å« 'traj_datasets'")
            invalid_args.append("experiment_name")
        
    # æ£€æŸ¥ recipe_tag_inputæ˜¯å¦ä¸ºç©ºï¼Œå¦‚æœä¸ºç©ºï¼Œåˆ™æŠ¥é”™
    if not args.recipe_tag_input:
        logger.warning(f"âŒ å‚æ•° recipe_tag_input ä¸èƒ½ä¸ºç©ºï¼Œå¿…é¡»æä¾›")
        invalid_args.append("recipe_tag_input")
    # æ£€æŸ¥ recipe_tag_outputæ˜¯å¦ä¸ºç©ºï¼Œå¦‚æœä¸ºç©ºï¼Œåˆ™æŠ¥é”™
    if not args.recipe_tag_output:
        logger.warning(f"âŒ å‚æ•° recipe_tag_output ä¸èƒ½ä¸ºç©ºï¼Œå¿…é¡»æä¾›")
        invalid_args.append("recipe_tag_output")
    # recipe_tag_outputä¸èƒ½å’Œ recipe_tag_input ç›¸åŒï¼Œå¹¶ä¸”å¿…é¡»ä»¥recipt_tag_inputå¼€å¤´
    if args.recipe_tag_output == args.recipe_tag_input:
        logger.warning(f"âŒ å‚æ•° recipe_tag_output {args.recipe_tag_output} å’Œ recipe_tag_input {args.recipe_tag_input} ä¸èƒ½ç›¸åŒ")
        invalid_args.append("recipe_tag_output")

    # input_path é‡Œå¿…é¡»åŒ…å« recipe_tag_input çš„å­å­—ç¬¦ä¸²
    if args.recipe_tag_input not in Path(args.input_path).parts:
        logger.warning(f"âŒ å‚æ•° input_path {args.input_path} è·¯å¾„ä¸­å¿…é¡»åŒ…å« recipe_tag_input: {args.recipe_tag_input}")
        invalid_args.append("input_path")

    # output_path ä¸èƒ½å’Œ input_path ç›¸åŒ
    if args.input_path == args.output_path:
        logger.warning(f"âŒ å‚æ•° output_path ä¸èƒ½å’Œ input_path ç›¸åŒ: {args.output_path}")
        invalid_args.append("output_path")

    return invalid_args

from obs import GetObjectHeader
from obs import ObsClient
import os
import base64

def download_obs_files(bucketName, objectPath, localDir='/root/.cache/'):
    ak = 'IP5YP0WXOJG4MAUBZEEP'
    sk = 'RFQ2Mjd3eXRBTTNKWVdGSWxPc2JoY2taNmxhbTVDMEg4Rlk4R1FHNg=='
    sk = base64.b64decode(sk).decode('utf-8')

    server = "https://obs.cn-east-3.myhuaweicloud.com"
    obsClient = ObsClient(access_key_id=ak, secret_access_key=sk, server=server)
    try:
        headers = GetObjectHeader()
        headers.if_modified_since = 'date'
        
        # æŠŠobjectPathé‡Œé¢çš„å¯¹è±¡æ‰¹é‡ä¸‹è½½åˆ°æœ¬åœ°æŒ‡å®šç›®å½•
        resp = obsClient.listObjects(bucketName, prefix=objectPath)
        if resp.status < 300:
            print('List Objects Succeeded')
            for content in resp.body.contents:
                objectKey = content.key
                print('Downloading object:', objectKey)
                downloadPath = os.path.join(localDir, objectKey)
                os.makedirs(os.path.dirname(downloadPath), exist_ok=True)
                getResp = obsClient.getObject(bucketName, objectKey, downloadPath, headers=headers)
                if getResp.status < 300:
                    print('Get Object Succeeded for', objectKey)
                else:
                    print('Get Object Failed for', objectKey)
                    print('requestId:', getResp.requestId)
                    print('errorCode:', getResp.errorCode)
                    print('errorMessage:', getResp.errorMessage)
        else:
            print('List Objects Failed')
            print('requestId:', resp.requestId)
            print('errorCode:', resp.errorCode)
            print('errorMessage:', resp.errorMessage)
    except:
        print('Operation Failed')

# filter_ids_pathä¸‹é¢æœ‰Nä¸ªæ–‡ä»¶
# æ¯ä¸ªæ–‡ä»¶çš„æ¯è¡Œå¯¹åº”{scene_name}|{uuid_array}ï¼Œå…¶ä¸­uuid_arrayæ˜¯é€—å·åˆ†éš”çš„uuidåˆ—è¡¨
# æ¯ä¸ªåœºæ™¯åªæœ‰ä¸€ä¸ªæ–‡ä»¶åŒ…å«å¯¹åº”çš„scene_nameï¼Œå¦‚æœè¯»å–åˆ°å¯¹åº”çš„scene_nameï¼Œåˆ™è¿”å›å¯¹åº”çš„uuid_array

def abstract_filter_ids(filter_ids_path, target_scene_name):
    bucketName = "data-platform-shanghai"
    objectPath = filter_ids_path.replace('obs://data-platform-shanghai/', '')
    download_obs_files(bucketName, objectPath, localDir='/root/.cache/')
    filter_ids_path = '/root/.cache/' + objectPath

    for file_name in os.listdir(filter_ids_path):
        input_file_path = os.path.join(filter_ids_path, file_name)
        with open(input_file_path, 'r', encoding='utf-8') as f:
            for line in f:
                scene_name, filter_ids = line.strip().split("|")
                if target_scene_name == scene_name:
                    return filter_ids
    return ""


# argparse å‚æ•°è§£æ
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="åŸºäºè¿‡æ»¤å‡ºæ¥çš„idåˆ—è¡¨ï¼Œå¯¼å‡ºå¯¹åº”çš„ObjectNavæ•°æ®é›†")
    parser.add_argument("--dataset_type", default="traj_datasets", help="task_datasets or traj_datasets")
    parser.add_argument("--input_path", default="data/traj_datasets/objectnav/cloudrobo_v1_l3mvn_all/train/content", help="è¾“å…¥æ•°æ®é›†è·¯å¾„")
    parser.add_argument("--recipe_tag_input", default="cloudrobo_v1_l3mvn_all", help="è¾“å…¥æ•°æ®é›†çš„recipe_tag")
    parser.add_argument("--recipe_tag_output", default="cloudrobo_v1_l3mvn", help="è¾“å‡ºæ•°æ®é›†çš„recipe_tag")
    parser.add_argument("--scene_name", default="shanghai-lianqiuhu-b11-4f-big-2025-07-15_11-40-37", help="åœºæ™¯åç§°")
    parser.add_argument("--filter_ids", default="null", help="è¿‡æ»¤çš„idåˆ—è¡¨ï¼Œé€—å·åˆ†éš”")
    parser.add_argument("--filter_ids_path", default="obs://data-platform-shanghai/dataarts/data-factory/jobs/job_2025_10_29_14_35_09_300_objectnav_traj_datasets_sql_query_batch_export_obs", help="è¿‡æ»¤çš„idåˆ—è¡¨æ–‡ä»¶è·¯å¾„, ä¼˜å…ˆçº§é«˜äºfilter_idså‚æ•°")
    parser.add_argument("--scene_type", default="cloudrobo_v1", help="åœºæ™¯ç±»å‹")
    parser.add_argument("--split", default="train", help="æ•°æ®é›†åˆ’åˆ†ç±»å‹")
    parser.add_argument("--overwrite", default="false", help="å¦‚æœè¾“å‡ºè·¯å¾„ä¸‹åœºæ™¯æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–")

    # è½¨è¿¹ç›¸å…³å‚æ•°
    parser.add_argument("--gen_traj_method", default="l3mvn", help="è½¨è¿¹ç”Ÿæˆæ–¹æ³•: sp(shortest_path) or hd(human_demonstration) or l3mvn")
    parser.add_argument("--experiment_name", default="null", help="å®éªŒåç§°")

    args = parser.parse_args()
    args.output_path = args.input_path.replace(args.recipe_tag_input, args.recipe_tag_output)

    invalid_args = precheck_args(args)
    if len(invalid_args) == 0:
        for arg, value in vars(args).items():
            logger.info(f"ğŸ”§ å‚æ•° {arg}: {value}")
        if args.filter_ids_path != 'null':
            args.filter_ids = abstract_filter_ids(args.filter_ids_path, args.scene_name)
            split_filter_ids = args.filter_ids.split(",")
            print(f"ğŸ” ä» filter_ids_path ä¸­è¯»å–åˆ°çš„è¿‡æ»¤ uuid æ•°é‡ä¸º: {len(split_filter_ids)}")
        main(args)
    elif len(invalid_args) > 0:
        for arg in invalid_args:
            logger.error(f"âŒ å‚æ•° {arg} æ— æ•ˆï¼Œè¯·æ£€æŸ¥åé‡æ–°è¿è¡Œè„šæœ¬ã€‚")
        sys.exit(1)